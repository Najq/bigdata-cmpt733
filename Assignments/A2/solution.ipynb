{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 4397038 pairs in total\n",
      "After Filtering: 743382 pairs left\n",
      "After Verification: 1823 similar pairs\n",
      "(precision, recall, fmeasure) =  (0.3987931980252331, 0.5592307692307692, 0.46557796990073647)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# similarity_join.py\n",
    "import re\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "class SimilarityJoin:\n",
    "    def __init__(self, data_file1, data_file2):\n",
    "        self.df1 = pd.read_csv(data_file1)\n",
    "        self.df2 = pd.read_csv(data_file2)\n",
    "         \n",
    "    def removeEmptystr(self,row):\n",
    "        emptyList= []\n",
    "        for val in row:\n",
    "            if val != \"\":\n",
    "                emptyList.append(val)\n",
    "        return emptyList\n",
    "        \n",
    "    def preprocess_df(self, df, cols): \n",
    "        \n",
    "        df = df.fillna('')\n",
    "        df['joinkey'] = df[cols].apply(lambda row : \" \".join(row.values.astype(str)), axis = 1 )\n",
    "        df['joinkey'] = df['joinkey'].apply(lambda row : re.split(r'\\W+', str(row).lower()))\n",
    "        #remove empty strings from Dataframe\n",
    "        df['joinkey'] = df['joinkey'].apply(self.removeEmptystr)\n",
    "        return df\n",
    "    \n",
    "    def splitListToRows(self,row,row_accumulator,target_column,new_column):\n",
    "        #create a new record with all the other values duplicated and one value from the joinkey column\n",
    "        split_row = row[target_column]\n",
    "        for s in split_row:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[new_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "    \n",
    "    def filtering(self, df1, df2):\n",
    "        \n",
    "        new_rows = []\n",
    "        df1.apply(self.splitListToRows,axis=1,args = (new_rows,\"joinkey\",\"flatten\"))\n",
    "        new_df1 = pd.DataFrame(new_rows)\n",
    "        new_rows = []\n",
    "        df2.apply(self.splitListToRows,axis=1,args = (new_rows,\"joinkey\",\"flatten\"))\n",
    "        new_df2 = pd.DataFrame(new_rows)\n",
    "        \n",
    "        df_inner = pd.merge(new_df1, new_df2, on='flatten', how='left')\n",
    "        df_inner = df_inner.drop(['flatten'],axis =1)\n",
    "        df_inner.drop_duplicates(subset=[\"id_x\",\"id_y\"], keep = \"first\", inplace = True) \n",
    "        df_inner = df_inner[['id_x','joinkey_x','id_y','joinkey_y']]\n",
    "        df_inner = df_inner.rename(columns={\"id_x\": \"id1\", \"joinkey_x\": \"joinKey1\", \"id_y\":\"id2\",\"joinkey_y\":\"joinKey2\"})\n",
    "        df_inner = df_inner.dropna()\n",
    "        #df_inner.to_csv (r'checking.csv', index = None, header=True)\n",
    "        return df_inner\n",
    "\n",
    "    def computesimilarity(self,row):\n",
    "        new_dict={}\n",
    "        for item in row:\n",
    "            if(item):\n",
    "                if item not in new_dict:\n",
    "                    new_dict[item] = 1\n",
    "                else:\n",
    "                    new_dict[item] += 1\n",
    "\n",
    "        rUs = len(new_dict.keys())\n",
    "        \n",
    "        return rUs\n",
    "                \n",
    "    def commonElements(self,row):\n",
    "        return len(set(row['joinKey1']) & set(row['joinKey2']))\n",
    "    \n",
    "    def verification(self, cand_df, threshold):\n",
    "        \n",
    "        #concate the joinkey columns\n",
    "        cand_df['concatedCol'] = cand_df['joinKey1'] + cand_df['joinKey2']\n",
    "        #calculate the unique elements in the concatenated col to get r U s\n",
    "        cand_df['rUs'] = cand_df['concatedCol'].apply(self.computesimilarity)\n",
    "        #find the intersection of r and s\n",
    "        cand_df['common'] = cand_df.apply(self.commonElements,axis=1)\n",
    "        cand_df['jaccard'] = cand_df['common']/cand_df['rUs']\n",
    "        \n",
    "        cand_df = cand_df[cand_df['jaccard'] >= threshold ]\n",
    "        cand_df = cand_df.drop(['rUs','common','concatedCol'],axis =1)\n",
    "        return cand_df\n",
    "        \n",
    "    def evaluate(self, result, ground_truth):\n",
    "        \n",
    "        R = len(result)\n",
    "        T= 0\n",
    "        for item in result:\n",
    "            if item in ground_truth:\n",
    "                T += 1\n",
    "        precision = T/R\n",
    "        \n",
    "        A = len(ground_truth)\n",
    "        recall = T/A\n",
    "        fmeasure = (2*precision*recall)/(precision + recall)\n",
    "        return (precision, recall, fmeasure)\n",
    "                \n",
    "        \n",
    "    def jaccard_join(self, cols1, cols2, threshold):\n",
    "        new_df1 = self.preprocess_df(self.df1, cols1)\n",
    "        new_df2 = self.preprocess_df(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.shape[0] *self.df2.shape[0])) \n",
    "        \n",
    "        cand_df = self.filtering(new_df1, new_df2)\n",
    "        print (\"After Filtering: %d pairs left\" %(cand_df.shape[0]))\n",
    "        \n",
    "        result_df = self.verification(cand_df, threshold)\n",
    "        print (\"After Verification: %d similar pairs\" %(result_df.shape[0]))\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    " \n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #path = \"/A2-data/A2-data/part1-similarity-join/Amazon-Google-Sample/\"\n",
    "    er = SimilarityJoin(\"Amazon.csv\", \"Google.csv\")\n",
    "    amazon_cols = [\"title\", \"manufacturer\"]\n",
    "    google_cols = [\"name\", \"manufacturer\"]\n",
    "    result_df = er.jaccard_join(amazon_cols, google_cols, 0.5)\n",
    "\n",
    "    result = result_df[['id1', 'id2']].values.tolist()\n",
    "    ground_truth = pd.read_csv(\"Amazon_Google_perfectMapping.csv\").values.tolist()\n",
    "    print (\"(precision, recall, fmeasure) = \", er.evaluate(result, ground_truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
