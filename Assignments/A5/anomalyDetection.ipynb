{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                features\n",
      "id                                                      \n",
      "0      [tcp, SF, -0.157817888091, 0.015677505748, -0....\n",
      "1      [tcp, SF, -0.158545578037, -0.0282581926784, -...\n",
      "2      [tcp, SF, -0.158545578037, -0.0261474606772, 0...\n",
      "3      [tcp, SF, -0.158545578037, 0.0132420457465, -0...\n",
      "4      [udp, SF, -0.158545578037, -0.0314405270803, -...\n",
      "...                                                  ...\n",
      "99090  [tcp, SF, -0.158545578037, -0.0240042558759, 0...\n",
      "99091  [udp, SF, 1.7552789774, -0.0301091422795, -0.1...\n",
      "99092  [tcp, SF, -0.158545578037, -0.0296545230793, -...\n",
      "99093  [tcp, S0, -0.158545578037, -0.0348501710824, -...\n",
      "99094  [tcp, SF, -0.158545578037, -0.0262448790772, -...\n",
      "\n",
      "[99095 rows x 1 columns]\n",
      "12\n",
      "                                                features\n",
      "id                                                      \n",
      "0      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "3      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "4      [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "...                                                  ...\n",
      "99090  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "99091  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "99092  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "99093  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n",
      "99094  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "\n",
      "[99095 rows x 1 columns]\n",
      "                                                features\n",
      "id                                                      \n",
      "0      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "2      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "3      [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "4      [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "...                                                  ...\n",
      "99090  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "99091  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "99092  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "99093  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...\n",
      "99094  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "\n",
      "[99095 rows x 1 columns]\n",
      "                                                features     score\n",
      "96835  [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.979265\n",
      "96836  [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.979265\n",
      "96837  [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.979265\n",
      "96838  [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.979265\n",
      "96839  [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  0.979265\n",
      "...                                                  ...       ...\n",
      "99090  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.000000\n",
      "99091  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.000000\n",
      "99092  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.000000\n",
      "99093  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.000000\n",
      "99094  [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  1.000000\n",
      "\n",
      "[2260 rows x 2 columns]\n",
      "2260\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "class AnomalyDetection():\n",
    "    \n",
    "        \n",
    "    def scaleNum(self, df, indices):\n",
    "         \n",
    "        #converting values to np array    \n",
    "        new_df= np.asarray(df.features.values.tolist()).astype(np.float)\n",
    "        #using vectorized operations to find mean and std\n",
    "        for pos in indices:\n",
    "            mean_val = new_df[:,pos].mean()\n",
    "            std_val = new_df[:,pos].std(ddof=1)\n",
    "            new_df[:,pos] = (new_df[:,pos] - mean_val)/(std_val)\n",
    "            \n",
    "            \n",
    "        dat = pd.DataFrame({'features_scaled': new_df.tolist()})\n",
    "        df['features']= dat['features_scaled']\n",
    "        return df\n",
    "\n",
    "\n",
    "    def cat2Num(self, df, indices):\n",
    "        \n",
    "        #converting lists to df to find unique\n",
    "        new_df= pd.DataFrame(df.features.values.tolist())\n",
    "        new_list = []\n",
    "        #finding unique features in the indices given\n",
    "        for val in indices:\n",
    "            new_list.extend(new_df[val].unique())\n",
    "        print(len(new_list))\n",
    "        target_col= np.asarray(df.features.values.tolist())\n",
    "        #making a vector of zeros based on length of df and unique features\n",
    "        row_vec = np.zeros((len(df),len(new_list)))\n",
    "        rest_of_features = target_col[:,len(indices):]\n",
    "        #making one hot encoding based on the features\n",
    "        for row in range(len(df)):\n",
    "            for val in indices:\n",
    "                if(target_col[row][val] in new_list):\n",
    "                    row_vec[row][new_list.index(target_col[row][val])] =1\n",
    "        #concatenating with the rest of the features\n",
    "        row_vec = np.concatenate((row_vec,rest_of_features),axis=1)\n",
    "        dat = pd.DataFrame({'features_oneHot': row_vec.tolist()})\n",
    "        df['features'] = dat['features_oneHot']\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "    def detect(self, df, k, t):\n",
    "        \n",
    "        X = np.array(df['features'].values.tolist())  \n",
    "        #kmeans = KMeans(n_clusters=k,init='k-means++').fit(X)\n",
    "        kmeans = KMeans(n_clusters=k,random_state=42).fit(X)\n",
    "        df['prediction'] =kmeans.predict(X)\n",
    "        \n",
    "        #grouping by predictions to get cluster counts\n",
    "        df_grouped = df.groupby(['prediction'],as_index=False).count()\n",
    "        \n",
    "        #join grouped by and original dataframe\n",
    "        df = pd.merge(df,df_grouped,how=\"inner\",on=\"prediction\",suffixes=('_x', '_y'))\n",
    "        preds = df_grouped['prediction'].values\n",
    "        value_counts = df_grouped['features'].values\n",
    "        cluster_counts = {}\n",
    "        for i in range(len(preds)):\n",
    "            cluster_counts[preds[i]] = value_counts[i]\n",
    "        #find min and max cluster size\n",
    "        max_cluster_size = max(cluster_counts.values())\n",
    "        min_cluster_size = min(cluster_counts.values())\n",
    "        #get score values\n",
    "        df['score'] = (max_cluster_size - df['features_y'])/(max_cluster_size-min_cluster_size)\n",
    "        df = df[df['score'] >= t]\n",
    "        df = df.rename(columns={'features_x':'features'})\n",
    "        df = df.drop(['prediction','features_y'],axis=1)\n",
    "        return df\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "#     data = [(0, [\"http\", \"udt\", 4]),\n",
    "#             (1, [\"http\", \"udf\", 5]),\n",
    "#             (2, [\"http\", \"tcp\", 5]),\n",
    "#             (3, [\"ftp\", \"icmp\", 1]),\n",
    "#             (4, [\"http\", \"tcp\", 4])]\n",
    "    df = pd.read_csv('A5-data/A5-data/logs-features-sample.csv').set_index('id')\n",
    "    df['features'] = df['features'].apply(literal_eval)\n",
    "#     df = pd.DataFrame(data=data, columns = [\"id\", \"features\"])\n",
    "    \n",
    "    ad = AnomalyDetection()\n",
    "    \n",
    "    print(df)\n",
    "    df1 = ad.cat2Num(df, [0,1])\n",
    "    print(df1)\n",
    "    #scaled the 13th feature for logs_sample_csv\n",
    "    df2 = ad.scaleNum(df1, [13])\n",
    "    print(df2)\n",
    "\n",
    "    df3 = ad.detect(df2, 8, 0.97)\n",
    "    print(df3)\n",
    "    print(len(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.2'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.17.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numpy.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
